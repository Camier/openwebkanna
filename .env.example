# ==============================================================================
# OpenWebUI with RAG - Environment Configuration Example
# ==============================================================================
# Copy this file to .env and modify the values as needed for your deployment.
# This configuration connects OpenWebUI to a LiteLLM proxy running in Docker.
#
# ==============================================================================
# Security Best Practices
# ==============================================================================
# - Pin all image versions for production deployments. Avoid `:latest` tags.
# - Regularly audit images with `trivy image <image:tag>` or similar scanners.
# - Update policy: address critical CVEs within 7 days of disclosure.
# - Keep secrets out of version control. Use .env (gitignored) for secrets.
# - Rotate API keys and tokens periodically, especially after any suspected exposure.

# ==============================================================================
# Authentication Settings
# ==============================================================================
# Set to true for multi-user deployments with authentication
# Set to false for single-user deployments without authentication
WEBUI_AUTH=true

# Display name for the OpenWebUI instance
WEBUI_NAME=OpenWebUI

# Secret key for session management.
# Must be >= 32 bytes. If you change this, all existing sessions/tokens become invalid (expected).
WEBUI_SECRET_KEY=

# Token used by the Jupyter sidecar and OpenWebUI Jupyter integration.
# Set a unique random value; this is required by docker-compose.yml.
JUPYTER_TOKEN=
# Jupyter container image. Use dated tags for reproducibility (e.g., 2026-02-01).
# Dated tags are published at https://hub.docker.com/r/jupyter/scipy-notebook/tags
# Example: JUPYTER_IMAGE=jupyter/scipy-notebook:2026-02-01
JUPYTER_IMAGE=jupyter/scipy-notebook:2025-01-13
JUPYTER_INTERNAL_PORT=8889
JUPYTER_URL=http://jupyter:8889
JUPYTER_PORT=8890
JUPYTER_EXECUTION_TIMEOUT=300
JUPYTER_ALLOW_CORS=true
JUPYTER_KERNEL_SPEC=python3
JUPYTER_CULL_IDLE_TIMEOUT=900
JUPYTER_CULL_INTERVAL=120
JUPYTER_CULL_CONNECTED=true
JUPYTER_CULL_BUSY=false
JUPYTER_SHUTDOWN_NO_ACTIVITY_TIMEOUT=1800
JUPYTER_TERMINALS_ENABLED=false
JUPYTER_LOG_LEVEL=INFO
JUPYTER_LAB_EXTENSION_MANAGER=readonly
JUPYTER_ALLOW_REMOTE_ACCESS=false
JUPYTER_LOCAL_HOSTNAMES=localhost,jupyter,openwebui_jupyter
JUPYTER_CPUS=2.0
JUPYTER_MEM_LIMIT=2g
JUPYTER_PIDS_LIMIT=512

# Port for OpenWebUI web interface
WEBUI_PORT=3000

# OpenWebUI container image (pin for reproducible deployments)
OPENWEBUI_IMAGE=ghcr.io/open-webui/open-webui:v0.8.3

# OpenWebUI resource limits
OPENWEBUI_CPUS=2.0
OPENWEBUI_MEM_LIMIT=4g
OPENWEBUI_PIDS_LIMIT=512

# PostgreSQL resource limits
POSTGRES_CPUS=1.0
POSTGRES_MEM_LIMIT=1g
POSTGRES_PIDS_LIMIT=256

# Disable Ollama by default for CLIProxyAPI-first deployments.
ENABLE_OLLAMA_API=true
# Ollama Cloud (remote Ollama host). Requires an API key configured in OpenWebUI:
# Admin Settings -> Connections -> Ollama -> Manage (wrench) -> API key.
# Docs: https://docs.ollama.com/cloud
OLLAMA_BASE_URL=https://ollama.com

# ==============================================================================
# LiteLLM Proxy (primary OpenAI-compatible upstream)
# ==============================================================================
# LiteLLM runs as an external Docker container on port 4000.
# OpenWebUI connects to it via host.docker.internal (host-gateway bridge).
# The LiteLLM master key is set in the litellm container environment.

OPENAI_API_BASE_URL=http://host.docker.internal:4000/v1
OPENAI_API_BASE_URLS=http://host.docker.internal:4000/v1

# LiteLLM master key — must match LITELLM_MASTER_KEY in the litellm container.
OPENAI_API_KEY=sk-litellm-replace-with-your-master-key
OPENAI_API_KEYS=sk-litellm-replace-with-your-master-key

# Default model name (optional — leave empty to auto-detect from /v1/models)
OPENAI_API_MODEL_NAME=

# Optional explicit chat model for scripts (e.g. test-rag.sh)
# If empty, scripts auto-detect first model from /v1/models
RAG_CHAT_MODEL=

# ==============================================================================
# CLIProxyAPI (optional sidecar — not the primary upstream)
# ==============================================================================
# CLIProxyAPI may still run alongside LiteLLM for OAuth-backed model aliases.
# OpenWebUI does NOT depend on it; set CLIPROXYAPI_ENABLED=false to fully disable.
CLIPROXYAPI_ENABLED=false
CLIPROXYAPI_DOCKER_MANAGED=true
CLIPROXYAPI_IMAGE=eceasy/cli-proxy-api:v6.8.18
CLIPROXYAPI_API_KEY=replace-with-strong-local-api-key
CLIPROXYAPI_BASE_URL=http://127.0.0.1:8317
CLIPROXYAPI_BIND_ADDRESS=127.0.0.1
CLIPROXYAPI_PORT=8317
CLIPROXYAPI_LOG_FILE=logs/cliproxyapi.log
CLIPROXYAPI_PID_FILE=.cliproxyapi.pid
CLIPROXYAPI_CPUS=0.5
CLIPROXYAPI_MEM_LIMIT=512m
CLIPROXYAPI_PIDS_LIMIT=128

# ==============================================================================
# MCPO (MCP-to-OpenAPI Proxy) Settings
# ==============================================================================
# OpenWebUI MCP support is Streamable HTTP. For stdio/SSE MCP servers,
# run MCPO and connect OpenWebUI External Tools to this proxy.
# Docs:
# - https://docs.openwebui.com/features/extensibility/mcp/
# - https://docs.openwebui.com/features/extensibility/plugin/tools/openapi-servers/mcp
# - https://github.com/open-webui/mcpo
# MCPO_IMAGE=ghcr.io/open-webui/mcpo:main
# Pinned to v0.0.19 for reproducible deployments (released 2025-10-14)
MCPO_IMAGE=ghcr.io/open-webui/mcpo:v0.0.19
MCPO_BIND_ADDRESS=127.0.0.1
MCPO_PORT=8000
MCPO_BASE_URL=http://127.0.0.1:8000
MCPO_CONFIG=./mcp/config.json
ZOTERO_DATA_DIR=/home/miko/Zotero
# Optional extra args appended to mcpo command. Keep default unless needed.
MCPO_ARGS=--host 0.0.0.0 --port 8000 --config /app/config.json

# MCPO resource limits
MCPO_CPUS=0.5
MCPO_MEM_LIMIT=512m
MCPO_PIDS_LIMIT=128

# Optional explicit base URL used by scripts.
OPENWEBUI_URL=http://localhost:3000

# ==============================================================================
# RAG (Retrieval-Augmented Generation) Settings
# ==============================================================================
# Embedding model for vectorizing documents
# Popular options:
# - sentence-transformers/all-MiniLM-L6-v2 (fast, lightweight, 384 dims)
# - sentence-transformers/all-mpnet-base-v2 (better quality, slower, 768 dims)
# - BAAI/bge-small-en-v1.5 (good quality for scientific content)
# - BAAI/bge-base-en-v1.5 (better quality, recommended for research)
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Number of document chunks to retrieve for context (increased for research depth)
RAG_TOP_K=15

# Number of chunks to return after reranking (if reranker enabled)
RAG_TOP_K_RERANKER=5

# Minimum relevance score for retrieved chunks (0.0-1.0)
RAG_RELEVANCE_THRESHOLD=0.3

# Include RAG context in system prompt
RAG_SYSTEM_CONTEXT=true

# Document chunking parameters (optimized for academic papers)
# Size of text chunks for document processing (in characters)
# Note: 3000 chars ≈ 750-1000 tokens, better for preserving context in scientific papers
CHUNK_SIZE=3000

# Overlap between adjacent chunks (in characters)
# 20% overlap recommended for context continuity
CHUNK_OVERLAP=600

# Hybrid search configuration (recommended for scientific/technical content)
# Combines vector similarity with BM25 keyword matching for better retrieval
ENABLE_RAG_HYBRID_SEARCH=true
ENABLE_RAG_HYBRID_SEARCH_ENRICHED_TEXTS=true
# Weight for BM25 in hybrid search (0.0-1.0, rest is vector similarity)
RAG_HYBRID_BM25_WEIGHT=0.3

# Vector database backend
# Options: chroma (default), faiss, pgvector (external PostgreSQL + pgvector extension)
VECTOR_DB=chroma

# PGVector connection settings (only used when VECTOR_DB=pgvector)
# This must point at a PostgreSQL instance with the `vector` extension available.
# Examples:
# PGVECTOR_DB_URL=postgresql://openwebui:change-me@host.docker.internal:5432/openwebui
# PGVECTOR_DB_URL=postgresql://openwebui:change-me@postgres:5432/openwebui
PGVECTOR_DB_URL=
# Attempt to create the `vector` extension automatically (requires sufficient DB privileges)
PGVECTOR_CREATE_EXTENSION=true
# Initialize the max expected vector length; should match your embedding dimensionality.
PGVECTOR_INITIALIZE_MAX_VECTOR_LENGTH=1536

# ==============================================================================
# Advanced RAG Settings
# ==============================================================================
# SearXNG host configuration (expected to be local on this machine)
SEARXNG_PORT=8888

# Web Search (SearXNG)
# OpenWebUI web search uses:
# - ENABLE_WEBSEARCH / WEBSEARCH_ENGINE / SEARXNG_QUERY_URL
# - WEB_SEARCH_RESULT_COUNT / WEB_SEARCH_CONCURRENT_REQUESTS
#
# Set this to your local SearXNG endpoint.
# Default assumes SearXNG is available at host.docker.internal:${SEARXNG_PORT}.
# Override if you run SearXNG on a different host/port.
ENABLE_WEB_SEARCH=false
ENABLE_WEBSEARCH=false
WEB_SEARCH_ENGINE=searxng
WEBSEARCH_ENGINE=searxng
WEB_SEARCH_RESULT_COUNT=3
WEB_SEARCH_CONCURRENT_REQUESTS=10
SEARXNG_QUERY_URL=http://host.docker.internal:${SEARXNG_PORT}/search?q={query}&format=json
SEARXNG_LANGUAGE=en

# SSL / CA bundles (optional)
# If HTTPS requests fail with SSL errors (corporate proxy / private CA), this repo
# mounts `./certs` into the OpenWebUI container at `/certs` (see docker-compose.yml).
#
# Examples:
#   REQUESTS_CA_BUNDLE=/certs/ca-bundle.pem
#   SSL_CERT_FILE=/certs/ca-bundle.pem
#   CURL_CA_BUNDLE=/certs/ca-bundle.pem
#
# Recommended: set REQUESTS_CA_BUNDLE first; only set SSL_CERT_FILE if needed.
#REQUESTS_CA_BUNDLE=/certs/ca-bundle.pem
#SSL_CERT_FILE=/certs/ca-bundle.pem
#CURL_CA_BUNDLE=/certs/ca-bundle.pem

# ==============================================================================
# Code Execution / Code Interpreter
# ==============================================================================
# ENABLE_CODE_EXECUTION / CODE_EXECUTION_ENGINE control in-chat code blocks.
# ENABLE_CODE_INTERPRETER / CODE_INTERPRETER_ENGINE enable LLM-driven interpreter mode.
# For Jupyter-backed execution, keep AUTH=token and set *_AUTH_TOKEN to JUPYTER_TOKEN.
ENABLE_CODE_EXECUTION=true
CODE_EXECUTION_ENGINE=jupyter
CODE_EXECUTION_JUPYTER_URL=http://jupyter:8889
CODE_EXECUTION_JUPYTER_AUTH=token
# Set to the same value as JUPYTER_TOKEN.
CODE_EXECUTION_JUPYTER_AUTH_TOKEN=
CODE_EXECUTION_JUPYTER_AUTH_PASSWORD=
CODE_EXECUTION_JUPYTER_TIMEOUT=60
ENABLE_CODE_INTERPRETER=true
CODE_INTERPRETER_ENGINE=jupyter
CODE_INTERPRETER_JUPYTER_URL=http://jupyter:8889
CODE_INTERPRETER_JUPYTER_AUTH=token
# Set to the same value as JUPYTER_TOKEN.
CODE_INTERPRETER_JUPYTER_AUTH_TOKEN=
CODE_INTERPRETER_JUPYTER_AUTH_PASSWORD=
CODE_INTERPRETER_JUPYTER_TIMEOUT=60
CODE_INTERPRETER_BLACKLISTED_MODULES=

# ==============================================================================
# Additional Optional Settings
# ==============================================================================
# Enable debug logging
# DEBUG=false

# Maximum upload size for files (in MB)
# MAX_UPLOAD_SIZE=100

# Enable image generation support
# ENABLE_IMAGE_GENERATION=false

# Default theme
# THEME=dark

# ==============================================================================
# LLM Council (Multi-Model Evaluation)
# ==============================================================================
# LLM Council runs multi-model benchmarks with candidate/judge pattern.
# Candidates generate responses, judges vote on the best answer.
# Usage: ./llm-council.sh --models "glm-5 minimax/chat-elite" --prompt "..."
# See README.md and API_EXAMPLES.md for detailed usage.

# Candidate models (space-separated) - models that generate responses
# Preferred MiniMax profile:
# - minimax/chat-elite (recommended)
# - minimax/chat-thinking / minimax/chat-quality retained for legacy callers
COUNCIL_MODELS=glm-5 minimax/chat-elite

# Judge models (space-separated) - models that evaluate and vote
# Defaults to COUNCIL_MODELS if not set
COUNCIL_JUDGES=

# Default prompt for evaluation (can be overridden via CLI)
COUNCIL_PROMPT=Summarize in 3 bullet points why retrieval-augmented generation improves reliability.

# File containing prompts (one per line, # for comments)
COUNCIL_PROMPTS_FILE=

# Output directory for markdown reports
COUNCIL_OUTPUT_DIR=logs/llm-council

# Candidate generation settings
COUNCIL_MAX_TOKENS=256
COUNCIL_TEMPERATURE=0.2

# Judge evaluation settings
COUNCIL_JUDGE_MAX_TOKENS=96
COUNCIL_JUDGE_RETRIES=2
COUNCIL_JUDGE_FORCE_MAX_TOKENS=96

# Judge output format: "winner" (WINNER=N) or "json" ({"winner": N})
# JSON format is stricter but more reliable for parsing
COUNCIL_JUDGE_OUTPUT_FORMAT=json

# Enable anti-position-bias voting (A/B then B/A) for 2-model evaluations
COUNCIL_POSITION_SWAP=false

# Request timeout in seconds
COUNCIL_TIMEOUT=120

# Include full candidate responses in report
COUNCIL_INCLUDE_RESPONSES=true

# ==============================================================================
# CI/CD and Security Scanning
# ==============================================================================
# Enable container security scanning in CI pipelines (e.g., trivy, grype).
# When enabled, CI should fail on critical/high CVEs found in images.
SECURITY_SCAN_ENABLED=true

# Enable OpenWebUI version checking at startup.
# When true, OpenWebUI will notify if a newer version is available.
# Disable for air-gapped or offline deployments.
OPENWEBUI_VERSION_CHECK=true
